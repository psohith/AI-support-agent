from typing import List, Dict, Any
import pandas as pd
from pydantic import BaseModel, Field
from langchain.schema import HumanMessage, AIMessage

from database.pgVectorDB.vector_store import VectorStore
from services.llm_factory import LLMFactory

class SynthesizedResponse(BaseModel):
    """
    A model representing the synthesized response generated by the AI assistant.

    Attributes:
        thought_process (List[str]): A list of thought processes the AI had while generating the response.
        answer (str): The synthesized answer to the user's question.
        enough_context (bool): Indicates if sufficient context was available to answer the question.
    """
    thought_process: List[str] = Field(
        description="Steps or reasoning by the AI assistant during response generation"
    )
    answer: str = Field(description="The assistant's response to the query")
    enough_context: bool = Field(
        description="Whether the assistant had adequate context to respond"
    )

class RAGModel:
    """
    A class implementing a Retrieval-Augmented Generation (RAG) model for answering user queries.

    Attributes:
        llm (Any): The Language Model instance used for generating responses.
        vectorStore (VectorStore): The vector store instance for retrieving context.
    """

    INITIAL_PROMPT_TEMPLATE = """
        You are a knowledgeable and professional customer support assistant specializing in the Testsigma testing product. 
        Your primary objective is to provide accurate, clear, and user-focused answers using the provided context. 

        Guidelines:
        1. Use only the supplied context for crafting responses.
        2. Reference URLs if they were there in the context.
        3. If the context is insufficient, recommend contacting customer support with professionalism.
        4. Maintain a professional and approachable tone throughout.
        5. Avoid speculating or providing unsupported information.
        6. Ensure your response is clear, actionable, and addresses the user's question directly.

        User Question: {question}

        Provided Context: {context}
    """

    PROMPT_TEMPLATE = """
        Based on the provided context and any prior interactions, craft a clear, concise, and professional response to the user's question.

        Guidelines:
        1. Use the retrieved context and any relevant details from previous questions or responses to ensure continuity and accuracy.
        2. Avoid speculation or providing information not explicitly mentioned in the context.
        3. If additional information or action is required, guide the user appropriately.

        User Question: {question}

        Provided Context: {context}
    """

    def __init__(self, provider: str):
        """
        Initialize the RAGModel instance.

        Args:
            provider (str): The LLM provider to use.
        """
        self.llm = LLMFactory(provider)
        self.vectorStore = VectorStore()

    def generate_response(self, question: str, chat_history: List[Dict[str, Any]] = None) -> SynthesizedResponse:
        """
        Generate a synthesized response based on the question and context.

        Args:
            question (str): The user's question.
            chat_history (List[Dict[str, str]], optional): The chat history as a list of dictionaries containing "question" and "response". Defaults to None.

        Returns:
            SynthesizedResponse: The response containing the AI's thought process, answer, and whether enough context was available.
        """
        chat_history = chat_history or []

        # Retrieve context from the vector store
        context_df = self.vectorStore.search(question, limit=4)
        context_str = self.dataframe_to_json(context_df, columns_to_keep=["content", "url"])

        # Prepare messages for the LLM
        messages = self.prepare_messages(question, context_str, chat_history)

        # Generate and return the response
        return self.llm.create_completion(
            response_model=SynthesizedResponse,
            messages=messages,
        )

    def prepare_messages(self, question: str, context: str, chat_history: List[Dict[str, str]]) -> List[Any]:
        """
        Prepare a list of messages for the LLM based on chat history and the current question.

        Args:
            question (str): The user's question.
            context (str): The retrieved context as a JSON string.
            chat_history (List[Dict[str, str]]): The chat history as a list of dictionaries.

        Returns:
            List[Any]: A list of HumanMessage and AIMessage instances.
        """
        messages = []

        for chat in chat_history:
            human_msg, ai_msg = self.chat_to_msg(chat)
            messages.append(human_msg)
            messages.append(ai_msg)

        if not messages:
            messages.append(HumanMessage(content=self.INITIAL_PROMPT_TEMPLATE.format(question=question, context=context)))
        else:
            messages.append(HumanMessage(content=self.PROMPT_TEMPLATE.format(question=question, context=context)))

        return messages

    @staticmethod
    def chat_to_msg(chat: Dict[str, str]) -> tuple:
        """
        Convert a chat entry to HumanMessage and AIMessage instances.

        Args:
            chat (Dict[str, str]): A dictionary containing "question" and "response".

        Returns:
            tuple: A tuple of HumanMessage and AIMessage instances.
        """
        human_msg = HumanMessage(content=chat["question"])
        ai_msg = AIMessage(content=chat["response"])
        return human_msg, ai_msg

    @staticmethod
    def dataframe_to_json(context: pd.DataFrame, columns_to_keep: List[str]) -> str:
        """
        Convert the context DataFrame to a JSON string.

        Args:
            context (pd.DataFrame): The context DataFrame.
            columns_to_keep (List[str]): The columns to include in the output.

        Returns:
            str: A JSON string representation of the selected columns.
        """
        return context[columns_to_keep].to_json(orient="records", indent=2)
